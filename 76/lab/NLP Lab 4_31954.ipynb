{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> PROBABILISTIC LANGUAGE MODELS </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main idea is to compute the probability of a sentence or sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been driving my car all night long. I do not know when exactly did I miss my exit. But, I am QUITE sure I was paying attention to the road.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I have been driving my car all night long.\\\n",
    " I do not know when exactly did I miss my exit.\\\n",
    " But, I am QUITE sure I was paying attention to the road.'\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'have')\n",
      "('have', 'been')\n",
      "('been', 'driving')\n",
      "('driving', 'my')\n",
      "('my', 'car')\n",
      "('car', 'all')\n",
      "('all', 'night')\n",
      "('night', 'long.')\n",
      "('long.', 'I')\n",
      "('I', 'do')\n",
      "('do', 'not')\n",
      "('not', 'know')\n",
      "('know', 'when')\n",
      "('when', 'exactly')\n",
      "('exactly', 'did')\n",
      "('did', 'I')\n",
      "('I', 'miss')\n",
      "('miss', 'my')\n",
      "('my', 'exit.')\n",
      "('exit.', 'But,')\n",
      "('But,', 'I')\n",
      "('I', 'am')\n",
      "('am', 'QUITE')\n",
      "('QUITE', 'sure')\n",
      "('sure', 'I')\n",
      "('I', 'was')\n",
      "('was', 'paying')\n",
      "('paying', 'attention')\n",
      "('attention', 'to')\n",
      "('to', 'the')\n",
      "('the', 'road.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "for bi in bigrams(text.split()):\n",
    "    print(bi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'have', 'been')\n",
      "('have', 'been', 'driving')\n",
      "('been', 'driving', 'my')\n",
      "('driving', 'my', 'car')\n",
      "('my', 'car', 'all')\n",
      "('car', 'all', 'night')\n",
      "('all', 'night', 'long.')\n",
      "('night', 'long.', 'I')\n",
      "('long.', 'I', 'do')\n",
      "('I', 'do', 'not')\n",
      "('do', 'not', 'know')\n",
      "('not', 'know', 'when')\n",
      "('know', 'when', 'exactly')\n",
      "('when', 'exactly', 'did')\n",
      "('exactly', 'did', 'I')\n",
      "('did', 'I', 'miss')\n",
      "('I', 'miss', 'my')\n",
      "('miss', 'my', 'exit.')\n",
      "('my', 'exit.', 'But,')\n",
      "('exit.', 'But,', 'I')\n",
      "('But,', 'I', 'am')\n",
      "('I', 'am', 'QUITE')\n",
      "('am', 'QUITE', 'sure')\n",
      "('QUITE', 'sure', 'I')\n",
      "('sure', 'I', 'was')\n",
      "('I', 'was', 'paying')\n",
      "('was', 'paying', 'attention')\n",
      "('paying', 'attention', 'to')\n",
      "('attention', 'to', 'the')\n",
      "('to', 'the', 'road.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "for tri in trigrams(text.split()):\n",
    "    print(tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a trigram Probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'have', 'been', 'driving', 'my', 'car', 'all', 'night', 'long', '.'],\n",
       " ['i',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'when',\n",
       "  'exactly',\n",
       "  'did',\n",
       "  'i',\n",
       "  'miss',\n",
       "  'my',\n",
       "  'exit',\n",
       "  '.'],\n",
       " ['but',\n",
       "  ',',\n",
       "  'i',\n",
       "  'am',\n",
       "  'quite',\n",
       "  'sure',\n",
       "  'i',\n",
       "  'was',\n",
       "  'paying',\n",
       "  'attention',\n",
       "  'to',\n",
       "  'the',\n",
       "  'road',\n",
       "  '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize \n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the built-in Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "model = MLE(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(model.counts['i']) # i.e. Count('i')\n",
    "print(model.counts[['have']]['been']) # i.e. Count('been'|'have')\n",
    "print(model.counts[['paying', 'attention']]['to']) # i.e. Count('to'|'paying attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(model.score('was','i'.split())) # i.e. Count('i')\n",
    "print(model.score('to', 'paying attention'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(was|I) = count(I|was) / count(I) = 1 / 5 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part-of-speech tags are used for grammar analysis and word sense disambiguation.\n",
    "For example, the word duck could refer to a bird, or it could be a verb indicating a downward\n",
    "motion. Computers cannot know the difference without additional information, such\n",
    "as part-of-speech tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default tagger to tag and untag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'),\n",
       " ('World', 'NN'),\n",
       " ('is', 'NN'),\n",
       " ('quite', 'NN'),\n",
       " ('literally', 'NN'),\n",
       " ('in', 'NN'),\n",
       " ('every', 'NN'),\n",
       " ('CS', 'NN'),\n",
       " ('Tutorial', 'NN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "tagger.tag(['Hello', 'World','is','quite','literally','in','every','CS','Tutorial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag([('Hello', 'NN'), ('World', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built in pos_tag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('boy', 'NN'), ('ate', 'VB'), ('the', 'DT'), ('amazingly', 'RB'), ('delicious', 'JJ'), ('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "print(pos_tag(word_tokenize('The boy ate the amazingly delicious cake')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the english meaning of the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective\n",
      "    ecent over-all possible hard-fought favorable hard meager fit such\n",
      "    widespread outmoded inadequate ambiguous grand clerical effective\n",
      "    orderly federal foster general proportionate ...\n"
     ]
    }
   ],
   "source": [
    "from nltk import help\n",
    "help.brown_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]\n",
      "Number of sents in brown corpus 57340\n",
      "Number of tokens in brown corpus 1161192\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "all_tagged = brown.tagged_sents()\n",
    "print(all_tagged[0:2]) # The first two sentences in the tagged corpus: List of a list of (word, tag) pairs\n",
    "print('Number of sents in brown corpus', len(all_tagged))\n",
    "print('Number of tokens in brown corpus', sum([len(sent) for sent in all_tagged]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting tagged sentences into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_tagged[:500] # First 500 sentences for training\n",
    "test = all_tagged[500:] # Rest for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the classifier based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ClassifierBasedPOSTagger class uses classification to do part-of-speech tagging.\n",
    "Features are extracted from words, and then passed to an internal classifier. The classifier\n",
    "classifies the features and returns a label, in this case, a part-of-speech tag.\n",
    "\n",
    "***PS: RUNNING THIS NEXT CELL IS GOING TO TAKE A WHILE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing training corpus for classifier.\n",
      "Training classifier (11711 instances)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7795781383996306"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger = ClassifierBasedPOSTagger(train=train, verbose=True)\n",
    "tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier on a random sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PPS'), ('watched', 'VBD'), ('the', 'AT'), ('play', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag(word_tokenize('He watched the play')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Build a classifier based POS tagger for the gutenberg corups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
